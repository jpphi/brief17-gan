{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "different-console",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#import os\n",
    "#import gdown\n",
    "#from zipfile import ZipFile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "about-scoop",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 500 files belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "dataset = keras.preprocessing.image_dataset_from_directory(\n",
    "    \"b17datas/test\", label_mode=None, image_size=(64, 64), batch_size=32\n",
    ")\n",
    "\n",
    "# -----> Pourquoi cette ligne ?\n",
    "dataset = dataset.map(lambda x: x / 255.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "electrical-print",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MapDataset shapes: (None, 64, 64, 3), types: tf.float32>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "abroad-university",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for x in dataset:\n",
    "    # -----> Pourquoi cette transformation ?\n",
    "    plt.imshow((x.numpy() * 255).astype(\"int32\")[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "natural-identifier",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"discriminator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 32, 32, 64)        3136      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)      (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 16, 16, 128)       131200    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 8, 8, 128)         262272    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 8193      \n",
      "=================================================================\n",
      "Total params: 404,801\n",
      "Trainable params: 404,801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "discriminator = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=(64, 64, 3)),\n",
    "        layers.Conv2D(64, kernel_size=4, strides=2, padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Conv2D(128, kernel_size=4, strides=2, padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Conv2D(128, kernel_size=4, strides=2, padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Flatten(),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense(1, activation=\"sigmoid\"),\n",
    "    ],\n",
    "    name=\"discriminator\",\n",
    ")\n",
    "discriminator.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "painted-headset",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "unsigned-tyler",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"generator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 8192)              1056768   \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose (Conv2DTran (None, 16, 16, 128)       262272    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 32, 32, 256)       524544    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 32, 32, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTr (None, 64, 64, 512)       2097664   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 64, 64, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 64, 64, 3)         38403     \n",
      "=================================================================\n",
      "Total params: 3,979,651\n",
      "Trainable params: 3,979,651\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "latent_dim = 128\n",
    "\n",
    "generator = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=(latent_dim,)),\n",
    "        layers.Dense(8 * 8 * 128),\n",
    "        layers.Reshape((8, 8, 128)),\n",
    "        layers.Conv2DTranspose(128, kernel_size=4, strides=2, padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Conv2DTranspose(256, kernel_size=4, strides=2, padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Conv2DTranspose(512, kernel_size=4, strides=2, padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Conv2D(3, kernel_size=5, padding=\"same\", activation=\"sigmoid\"),\n",
    "    ],\n",
    "    name=\"generator\",\n",
    ")\n",
    "generator.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bound-bullet",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "convertible-deputy",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN(keras.Model):\n",
    "    def __init__(self, discriminator, generator, latent_dim):\n",
    "        super(GAN, self).__init__()\n",
    "        self.discriminator = discriminator\n",
    "        self.generator = generator\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "    def compile(self, d_optimizer, g_optimizer, loss_fn):\n",
    "        super(GAN, self).compile()\n",
    "        self.d_optimizer = d_optimizer\n",
    "        self.g_optimizer = g_optimizer\n",
    "        self.loss_fn = loss_fn\n",
    "        self.d_loss_metric = keras.metrics.Mean(name=\"d_loss\")\n",
    "        self.g_loss_metric = keras.metrics.Mean(name=\"g_loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.d_loss_metric, self.g_loss_metric]\n",
    "\n",
    "    def train_step(self, real_images):\n",
    "        # Sample random points in the latent space\n",
    "        batch_size = tf.shape(real_images)[0]\n",
    "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "\n",
    "        # Decode them to fake images\n",
    "        generated_images = self.generator(random_latent_vectors)\n",
    "\n",
    "        # Combine them with real images\n",
    "        combined_images = tf.concat([generated_images, real_images], axis=0)\n",
    "\n",
    "        # Assemble labels discriminating real from fake images\n",
    "        labels = tf.concat(\n",
    "            [tf.ones((batch_size, 1)), tf.zeros((batch_size, 1))], axis=0\n",
    "        )\n",
    "        # Add random noise to the labels - important trick!\n",
    "        labels += 0.05 * tf.random.uniform(tf.shape(labels))\n",
    "\n",
    "        # Train the discriminator\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self.discriminator(combined_images)\n",
    "            d_loss = self.loss_fn(labels, predictions)\n",
    "        grads = tape.gradient(d_loss, self.discriminator.trainable_weights)\n",
    "        self.d_optimizer.apply_gradients(\n",
    "            zip(grads, self.discriminator.trainable_weights)\n",
    "        )\n",
    "\n",
    "        # Sample random points in the latent space\n",
    "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "\n",
    "        # Assemble labels that say \"all real images\"\n",
    "        misleading_labels = tf.zeros((batch_size, 1))\n",
    "\n",
    "        # Train the generator (note that we should *not* update the weights\n",
    "        # of the discriminator)!\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self.discriminator(self.generator(random_latent_vectors))\n",
    "            g_loss = self.loss_fn(misleading_labels, predictions)\n",
    "        grads = tape.gradient(g_loss, self.generator.trainable_weights)\n",
    "        self.g_optimizer.apply_gradients(zip(grads, self.generator.trainable_weights))\n",
    "\n",
    "        # Update metrics\n",
    "        self.d_loss_metric.update_state(d_loss)\n",
    "        self.g_loss_metric.update_state(g_loss)\n",
    "        return {\n",
    "            \"d_loss\": self.d_loss_metric.result(),\n",
    "            \"g_loss\": self.g_loss_metric.result(),\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sorted-magnet",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "structural-sheet",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GANMonitor(keras.callbacks.Callback):\n",
    "    def __init__(self, num_img=3, latent_dim=128):\n",
    "        self.num_img = num_img\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        random_latent_vectors = tf.random.normal(shape=(self.num_img, self.latent_dim))\n",
    "        generated_images = self.model.generator(random_latent_vectors)\n",
    "        generated_images *= 255\n",
    "        generated_images.numpy()\n",
    "        for i in range(self.num_img):\n",
    "            img = keras.preprocessing.image.array_to_img(generated_images[i])\n",
    "            img.save(\"generated_img_%03d_%d.png\" % (epoch, i))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "first-sellers",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "visible-detection",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "16/16 [==============================] - 250s 15s/step - d_loss: 0.6329 - g_loss: 0.5992\n",
      "Epoch 2/5\n",
      "16/16 [==============================] - 259s 16s/step - d_loss: 0.4609 - g_loss: 0.8985\n",
      "Epoch 3/5\n",
      "16/16 [==============================] - 281s 17s/step - d_loss: 0.2344 - g_loss: 1.6033\n",
      "Epoch 4/5\n",
      "16/16 [==============================] - 285s 18s/step - d_loss: 0.3054 - g_loss: 1.3933\n",
      "Epoch 5/5\n",
      "16/16 [==============================] - 249s 15s/step - d_loss: 0.5700 - g_loss: 0.6774\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9ef0ff3070>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 5  # In practice, use ~100 epochs\n",
    "\n",
    "gan = GAN(discriminator=discriminator, generator=generator, latent_dim=latent_dim)\n",
    "gan.compile(\n",
    "    d_optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n",
    "    g_optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n",
    "    loss_fn=keras.losses.BinaryCrossentropy(),\n",
    ")\n",
    "\n",
    "gan.fit(\n",
    "    dataset, epochs=epochs, callbacks=[GANMonitor(num_img=10, latent_dim=latent_dim)]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "coated-benchmark",
   "metadata": {},
   "outputs": [],
   "source": [
    "gan.save_weights(\"gan.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "multiple-excess",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deluxe-designation",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "requested-astronomy",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legislative-thomson",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "freelance-kitchen",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sunset-persian",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
