{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "photographic-opening",
   "metadata": {},
   "source": [
    "# Introduction to generative adversarial networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "valuable-button",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import layers\n",
    "import numpy as np\n",
    "\n",
    "latent_dim = 32\n",
    "height = 32\n",
    "width = 32\n",
    "channels = 3\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "funny-inspector",
   "metadata": {},
   "source": [
    "## Le générateur  \n",
    "Tout d'abord, développons un modèle générateur qui transforme un vecteur (de l'espace latent-\n",
    "pendant la formation, il sera échantillonné de manière aléatoire) en une image candidate. L'un des\n",
    "l'un des nombreux problèmes qui se posent couramment avec les GAN est que le générateur reste bloqué avec des images générées qui ressemblent à du bruit.\n",
    "images générées qui ressemblent à du bruit. Une solution possible est d'utiliser le dropout à la fois sur le dis- criminateur et le générateur.\n",
    "criminateur et le générateur.\n",
    "Listing 8.29 Générateur GAN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "divine-bundle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_14\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_22 (InputLayer)        [(None, 32)]              0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 32768)             1081344   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_59 (LeakyReLU)   (None, 32768)             0         \n",
      "_________________________________________________________________\n",
      "reshape_7 (Reshape)          (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_52 (Conv2D)           (None, 16, 16, 256)       819456    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_60 (LeakyReLU)   (None, 16, 16, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_6 (Conv2DTr (None, 32, 32, 256)       1048832   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_61 (LeakyReLU)   (None, 32, 32, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_53 (Conv2D)           (None, 32, 32, 256)       1638656   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_62 (LeakyReLU)   (None, 32, 32, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_54 (Conv2D)           (None, 32, 32, 256)       1638656   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_63 (LeakyReLU)   (None, 32, 32, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_55 (Conv2D)           (None, 32, 32, 3)         37635     \n",
      "=================================================================\n",
      "Total params: 6,264,579\n",
      "Trainable params: 6,264,579\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "generator_input = keras.Input(shape=(latent_dim,))\n",
    "x = layers.Dense(128 * 16 * 16)(generator_input)\n",
    "#x = layers.Dense(64 * 64 * 64)(generator_input)\n",
    "x = layers.LeakyReLU()(x)\n",
    "x = layers.Reshape((16, 16, 128))(x)\n",
    "#x = layers.Reshape((32, 32, 128))(x)\n",
    "\n",
    "# Transforms the input into a 16 × 16 128-channel feature map\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "x = layers.Conv2D(256, 5, padding='same')(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "x = layers.Conv2DTranspose(256, 4, strides=2, padding='same')(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "\n",
    "x= layers.Conv2D(256, 5, padding='same')(x)\n",
    "x= layers.LeakyReLU()(x)\n",
    "x= layers.Conv2D(256, 5, padding='same')(x)\n",
    "x= layers.LeakyReLU()(x)\n",
    "\n",
    "#Upsamples to 32 × 32\n",
    "x = layers.Conv2D(channels, 7, activation='tanh', padding='same')(x)\n",
    "\n",
    "generator = keras.models.Model(generator_input, x)\n",
    "generator.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subjective-richardson",
   "metadata": {},
   "source": [
    "## Le discriminateur\n",
    "Ensuite, vous allez développer un modèle de discriminateur qui prend en entrée une image candidate\n",
    "(réelle ou synthétique) et la classe dans l'une des deux catégories suivantes : \"image générée\" ou \"image réelle\n",
    "image réelle provenant de l'ensemble d'apprentissage \".\n",
    "Listing 8.30 Le réseau discriminant GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "lyric-aaron",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_15\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_23 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d_56 (Conv2D)           (None, 30, 30, 128)       3584      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_64 (LeakyReLU)   (None, 30, 30, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_57 (Conv2D)           (None, 14, 14, 128)       262272    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_65 (LeakyReLU)   (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_58 (Conv2D)           (None, 6, 6, 128)         262272    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_66 (LeakyReLU)   (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_59 (Conv2D)           (None, 2, 2, 128)         262272    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_67 (LeakyReLU)   (None, 2, 2, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 790,913\n",
      "Trainable params: 790,913\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "discriminator_input = layers.Input(shape=(height, width, channels))\n",
    "x = layers.Conv2D(128, 3)(discriminator_input)\n",
    "x = layers.LeakyReLU()(x)\n",
    "x = layers.Conv2D(128, 4, strides=2)(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "x = layers.Conv2D(128, 4, strides=2)(x)\n",
    "\n",
    "\n",
    "x = layers.LeakyReLU()(x)\n",
    "\n",
    "x = layers.Conv2D(128, 4, strides=2)(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "x = layers.Flatten()(x)\n",
    "\n",
    "x = layers.Dropout(0.4)(x) #One dropout layer: an important trick!\n",
    "x = layers.Dense(1, activation='sigmoid')(x) # Classification layer\n",
    "\n",
    "discriminator = keras.models.Model(discriminator_input, x)\n",
    "discriminator.summary()\n",
    "discriminator_optimizer = keras.optimizers.RMSprop(\n",
    "    lr=0.0008,\n",
    "    clipvalue=1.0,\n",
    "    decay=1e-8)\n",
    "\n",
    "discriminator.compile(optimizer=discriminator_optimizer,\n",
    "loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "tropical-responsibility",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "authentic-reasoning",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "relative-aerospace",
   "metadata": {},
   "source": [
    "# Le réseau adversatif\n",
    "Enfin, vous allez configurer le GAN , qui enchaîne le générateur et le discriminateur.\n",
    "Une fois entraîné, ce modèle va déplacer le générateur dans une direction qui améliore sa capacité à tromper le discriminateur.\n",
    "à tromper le discriminateur. Ce modèle transforme les points de l'espace latent en une décision de classification - \"faux\" ou \"vrai\".\n",
    "en une décision de classification - \"faux\" ou \"vrai\" - et il est censé être entraîné avec des étiquettes qui sont toujours \"ces images sont réelles\".\n",
    "\"ce sont de vraies images\". Donc, l'entraînement de gan va mettre à jour les poids du générateur d'une manière\n",
    "qui rend le discriminateur plus susceptible de prédire \"réel\" lorsqu'il regarde des images fausses.\n",
    "Il est très important de noter que vous avez configuré le discriminateur pour qu'il soit gelé pendant l'entraînement (non entraînable).\n",
    "(non-trainable) : ses poids ne seront pas mis à jour lors de l'apprentissage de gan . Si les poids du discriminateur\n",
    "Si les poids du discriminateur pouvaient être mis à jour au cours de ce processus, vous entraîneriez le discriminateur à toujours prédire les \" vrais \" résultats.\n",
    "Si les poids du discriminateur pouvaient être mis à jour au cours de ce processus, vous entraîneriez le discriminateur à toujours prédire \"réel\", ce qui n'est pas ce que vous voulez !\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "rolled-maria",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets discriminator weights to non-trainable (this will only apply to the gan model)\n",
    "\n",
    "discriminator.trainable = False \n",
    "\n",
    "gan_input = keras.Input(shape=(latent_dim,))\n",
    "gan_output = discriminator(generator(gan_input))\n",
    "gan = keras.models.Model(gan_input, gan_output)\n",
    "gan_optimizer = keras.optimizers.RMSprop(lr=0.0004, clipvalue=1.0, decay=1e-8)\n",
    "gan.compile(optimizer=gan_optimizer, loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "humanitarian-converter",
   "metadata": {},
   "source": [
    "WARNING:tensorflow:Model was constructed with shape (None, 128, 128, 3) for input KerasTensor(type_spec=TensorSpec(shape=(None, 128, 128, 3), dtype=tf.float32, name='input_2'), name='input_2', description=\"created by layer 'input_2'\"), but it was called on an input with incompatible shape (None, 32, 32, 3).\n",
    "\n",
    "WARNING:tensorflow:Model was constructed with shape (None, 64, 64, 3) for input KerasTensor(type_spec=TensorSpec(shape=(None, 64, 64, 3), dtype=tf.float32, name='input_12'), name='input_12', description=\"created by layer 'input_12'\"), but it was called on an input with incompatible shape (None, 32, 32, 3).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "paperback-bridal",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "younger-salmon",
   "metadata": {},
   "source": [
    "# Comment former votre DCGAN\n",
    "Vous pouvez maintenant commencer l'entraînement. Pour récapituler, voici à quoi ressemble la boucle d'entraînement\n",
    "schématiquement. Pour chaque époque, vous effectuez les opérations suivantes :\n",
    "1- Dessiner des points aléatoires dans l'espace latent (bruit aléatoire).\n",
    "2- Générer des images avec le générateur en utilisant ce bruit aléatoire.\n",
    "3- Mélanger les images générées avec des images réelles.\n",
    "4- Entraîner le discriminateur en utilisant ces images mélangées, avec les cibles correspondantes : soit \"vrai\" (pour les images réelles) soit \"faux\" (pour les images générées).\n",
    "5- Dessiner de nouveaux points aléatoires dans l'espace latent.\n",
    "6- Entraîner le gan en utilisant ces vecteurs aléatoires, avec des cibles qui disent toutes \"ce sont de vraies images\". Cela met à jour les poids du générateur (seulement, parce que le discriminateur est gelé à l'intérieur de gan) pour les faire évoluer vers la prédiction par le discriminateur de \"ce sont de vraies images\" pour les images générées : cela entraîne le générateur à tromper le discriminateur.\n",
    "Implémentons-le."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accepted-geology",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continental-level",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "applicable-recruitment",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "surgical-remedy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 500 files belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "dataset = keras.preprocessing.image_dataset_from_directory(\n",
    "    \"b17datas/test\", label_mode=None, image_size=(32, 32), batch_size=32\n",
    ")\n",
    "\n",
    "# -----> Pourquoi cette ligne ?\n",
    "dataset = dataset.map(lambda x: x / 255.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "matched-stake",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MapDataset shapes: (None, 32, 32, 3), types: tf.float32>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "involved-pregnancy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "x_train = np.empty(shape=(0,height, width, 3))\n",
    "\n",
    "for elt in dataset.as_numpy_iterator():\n",
    "    x_train = np.concatenate((x_train, elt), axis=0)\n",
    "      \n",
    "\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alpine-conversation",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "arctic-survivor",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import os\n",
    "#\n",
    "##from keras.preprocessing import image # Loads CIFAR10 data\n",
    "#\n",
    "#(x_train, y_train), (_, _) = keras.load_data(dataset)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fluid-lobby",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "signed-flashing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 32, 32, 3)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(type(x_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "clinical-franklin",
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_train = x_train[y_train.flatten() == 6] #Selects frog images (class 6)\n",
    "#\n",
    "##dataset = keras.preprocessing.image_dataset_from_directory(\"b17datas/test\", label_mode= None, image_size=(64, 64), batch_size=32)\n",
    "## -----> Pourquoi cette ligne ?\n",
    "##dataset = dataset.map(lambda x: x / 255.0)\n",
    "#\n",
    "#x_train = x_train.reshape((x_train.shape[0],) + (height, width, channels)).astype('float32') / 255. # Normalizes data\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "steady-legend",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "academic-official",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acceptable-prevention",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "german-cause",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "discriminator loss: 0.6878975033760071\n",
      "adversarial loss: 0.6593941450119019\n",
      "100\n",
      "discriminator loss: 0.5805875062942505\n",
      "adversarial loss: 1.3637793064117432\n",
      "200\n",
      "discriminator loss: 0.6007582545280457\n",
      "adversarial loss: 1.3806746006011963\n",
      "300\n",
      "discriminator loss: 0.6156712770462036\n",
      "adversarial loss: 0.7641630172729492\n",
      "400\n",
      "discriminator loss: 0.5897729396820068\n",
      "adversarial loss: 0.9305468797683716\n",
      "500\n",
      "discriminator loss: 0.5653040409088135\n",
      "adversarial loss: 0.5294497013092041\n",
      "600\n",
      "discriminator loss: 0.48168158531188965\n",
      "adversarial loss: 0.47028398513793945\n",
      "700\n",
      "discriminator loss: 0.5131665468215942\n",
      "adversarial loss: 2.3199706077575684\n",
      "800\n",
      "discriminator loss: 0.5401914715766907\n",
      "adversarial loss: 1.4528510570526123\n",
      "900\n",
      "discriminator loss: 0.5705994963645935\n",
      "adversarial loss: 0.8480575680732727\n"
     ]
    }
   ],
   "source": [
    "\n",
    "iterations = 1000\n",
    "batch_size = 20\n",
    "save_dir = 'sav_dir' # Specifies where you want to save generated images\n",
    "\n",
    "start = 0\n",
    "\n",
    "for step in range(iterations):\n",
    "    # Samples random points in the latent space\n",
    "    random_latent_vectors = np.random.normal(size=(batch_size,latent_dim))\n",
    "\n",
    "    generated_images = generator.predict(random_latent_vectors) # Decodes them to fake images\n",
    "\n",
    "    # 3 lignes suivantes: Combines them with real images\n",
    "    stop = start + batch_size\n",
    "    real_images = x_train[start: stop]\n",
    "    combined_images = np.concatenate([generated_images, real_images]) # Assembles labels, discriminating real from fake images\n",
    "\n",
    "\n",
    "    labels = np.concatenate([np.ones((batch_size, 1)),\n",
    "    np.zeros((batch_size, 1))])\n",
    "    labels += 0.05 * np.random.random(labels.shape) # Adds random noise to the labels—an important trick!\n",
    "\n",
    "\n",
    "    d_loss = discriminator.train_on_batch(combined_images, labels) # Trains the discriminator\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    random_latent_vectors = np.random.normal(size=(batch_size, latent_dim)) # Samples random points in the latent space\n",
    "\n",
    "    misleading_targets = np.zeros((batch_size, 1)) # Assembles labels that say “these are all real images” (it’s a lie!)\n",
    "\n",
    "\n",
    "\n",
    "    # Trains the generator (via the gan model, where the discriminator weights are frozen)\n",
    "    a_loss = gan.train_on_batch(random_latent_vectors, misleading_targets)\n",
    "\n",
    "    start += batch_size\n",
    "    if start > len(x_train) - batch_size:\n",
    "        start = 0\n",
    "\n",
    "    if step % 100 == 0:\n",
    "        print(step)\n",
    "\n",
    "        gan.save_weights('gan.h5') # Occasionally saves and plots (every 100 steps)\n",
    "\n",
    "        # Prints metrics\n",
    "        print('discriminator loss:', d_loss)\n",
    "        print('adversarial loss:', a_loss)\n",
    "\n",
    "        # Saves one generated image\n",
    "        img = image.array_to_img(generated_images[0] * 255., scale=False) \n",
    "        img.save(os.path.join(save_dir,'generated_frog' + str(step) + '.png'))\n",
    "\n",
    "        # Saves one real image for comparison\n",
    "        img = image.array_to_img(real_images[0] * 255., scale=False)\n",
    "        img.save(os.path.join(save_dir,'real_frog' + str(step) + '.png'))\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "single-store",
   "metadata": {},
   "source": [
    "0\n",
    "discriminator loss: 0.6903222799301147\n",
    "adversarial loss: 0.7277755737304688\n",
    "100\n",
    "discriminator loss: 0.694083571434021\n",
    "adversarial loss: 0.7294188737869263\n",
    "200\n",
    "discriminator loss: 0.7028545141220093\n",
    "adversarial loss: 0.7415596842765808\n",
    "300\n",
    "discriminator loss: 0.6862263679504395\n",
    "adversarial loss: 0.7502947449684143\n",
    "400\n",
    "discriminator loss: 0.691260039806366\n",
    "adversarial loss: 0.7984288930892944\n",
    "500\n",
    "discriminator loss: 0.7227819561958313\n",
    "adversarial loss: 0.7326540946960449\n",
    "600\n",
    "discriminator loss: 0.6846193075180054\n",
    "adversarial loss: 0.737768828868866\n",
    "700\n",
    "discriminator loss: 0.6955347657203674\n",
    "adversarial loss: 0.7362470626831055\n",
    "800\n",
    "discriminator loss: 0.6925490498542786\n",
    "adversarial loss: 1.191929578781128\n",
    "900\n",
    "discriminator loss: 0.9772886037826538\n",
    "adversarial loss: 0.7793024778366089\n",
    "1000\n",
    "discriminator loss: 0.7052279710769653\n",
    "adversarial loss: 0.7791701555252075\n",
    "1100\n",
    "discriminator loss: 0.6872953176498413\n",
    "adversarial loss: 0.7412459254264832\n",
    "1200\n",
    "discriminator loss: 0.7101338505744934\n",
    "adversarial loss: 0.7623135447502136\n",
    "1300\n",
    "discriminator loss: 0.6961228847503662\n",
    "adversarial loss: 0.7427016496658325\n",
    "1400\n",
    "discriminator loss: 0.7005981206893921\n",
    "adversarial loss: 0.7154887914657593\n",
    "1500\n",
    "discriminator loss: 0.6898554563522339\n",
    "adversarial loss: 0.7627737522125244\n",
    "1600\n",
    "discriminator loss: 0.6979285478591919\n",
    "adversarial loss: 0.7377437949180603\n",
    "1700\n",
    "discriminator loss: 0.7146362066268921\n",
    "adversarial loss: 0.676193356513977\n",
    "1800\n",
    "discriminator loss: 0.6964025497436523\n",
    "adversarial loss: 0.7423054575920105\n",
    "1900\n",
    "discriminator loss: 0.6941360831260681\n",
    "adversarial loss: 0.7726584672927856\n",
    "2000\n",
    "discriminator loss: 0.6925191283226013\n",
    "adversarial loss: 0.6859161853790283\n",
    "2100\n",
    "discriminator loss: 0.6875797510147095\n",
    "adversarial loss: 0.7810182571411133\n",
    "2200\n",
    "discriminator loss: 0.6915210485458374\n",
    "adversarial loss: 0.7604604959487915\n",
    "2300\n",
    "discriminator loss: 0.7092397809028625\n",
    "adversarial loss: 0.7542449831962585\n",
    "2400\n",
    "discriminator loss: 0.6928702592849731\n",
    "adversarial loss: 0.9263942837715149\n",
    "2500\n",
    "discriminator loss: 0.6910996437072754\n",
    "adversarial loss: 0.7976343035697937\n",
    "2600\n",
    "discriminator loss: 0.7138384580612183\n",
    "adversarial loss: 0.7331719994544983\n",
    "2700\n",
    "discriminator loss: 0.7069092988967896\n",
    "adversarial loss: 0.7194826006889343\n",
    "2800\n",
    "discriminator loss: 0.7702499628067017\n",
    "adversarial loss: 0.7816265821456909\n",
    "2900\n",
    "discriminator loss: 0.6872971057891846\n",
    "adversarial loss: 0.7678540349006653\n",
    "3000\n",
    "discriminator loss: 0.6789027452468872\n",
    "adversarial loss: 0.7489149570465088\n",
    "3100\n",
    "discriminator loss: 0.6898749470710754\n",
    "adversarial loss: 0.7497231364250183\n",
    "3200\n",
    "discriminator loss: 0.7014172673225403\n",
    "adversarial loss: 0.7262067794799805\n",
    "3300\n",
    "discriminator loss: 0.6933850049972534\n",
    "adversarial loss: 0.7308087348937988\n",
    "3400\n",
    "discriminator loss: 0.6901521682739258\n",
    "adversarial loss: 0.7862777709960938\n",
    "3500\n",
    "discriminator loss: 0.7048423886299133\n",
    "adversarial loss: 0.7689612507820129\n",
    "3600\n",
    "discriminator loss: 0.685257077217102\n",
    "adversarial loss: 0.7594449520111084\n",
    "3700\n",
    "discriminator loss: 0.6855848431587219\n",
    "adversarial loss: 0.7020666003227234\n",
    "3800\n",
    "discriminator loss: 0.6990402936935425\n",
    "adversarial loss: 0.8109185099601746\n",
    "3900\n",
    "discriminator loss: 0.7030068635940552\n",
    "adversarial loss: 0.752684473991394\n",
    "4000\n",
    "discriminator loss: 0.7026652097702026\n",
    "adversarial loss: 0.8994032144546509\n",
    "4100\n",
    "discriminator loss: 0.6956710815429688\n",
    "adversarial loss: 0.7235678434371948\n",
    "4200\n",
    "discriminator loss: 0.694584846496582\n",
    "adversarial loss: 0.7797266244888306\n",
    "4300\n",
    "discriminator loss: 0.7056791186332703\n",
    "adversarial loss: 0.8373244404792786\n",
    "4400\n",
    "discriminator loss: 0.7029595375061035\n",
    "adversarial loss: 0.7738478779792786\n",
    "4500\n",
    "discriminator loss: 0.7092242240905762\n",
    "adversarial loss: 1.9663206338882446\n",
    "4600\n",
    "discriminator loss: 0.6784204840660095\n",
    "adversarial loss: 0.7169632911682129\n",
    "4700\n",
    "discriminator loss: 0.686933159828186\n",
    "adversarial loss: 0.8063060641288757\n",
    "4800\n",
    "discriminator loss: 0.6839805841445923\n",
    "adversarial loss: 0.7516949772834778\n",
    "4900\n",
    "discriminator loss: 0.700552761554718\n",
    "adversarial loss: 1.0836750268936157\n",
    "5000\n",
    "discriminator loss: 0.6966218948364258\n",
    "adversarial loss: 0.7664633989334106\n",
    "5100\n",
    "discriminator loss: 0.6970637440681458\n",
    "adversarial loss: 0.7509037852287292\n",
    "5200\n",
    "discriminator loss: 0.7084998488426208\n",
    "adversarial loss: 0.756239116191864\n",
    "5300\n",
    "discriminator loss: 0.7205269932746887\n",
    "adversarial loss: 0.7650889158248901\n",
    "5400\n",
    "discriminator loss: 0.6898127794265747\n",
    "adversarial loss: 0.7067615389823914\n",
    "5500\n",
    "discriminator loss: 0.6919823884963989\n",
    "adversarial loss: 0.7605329751968384\n",
    "5600\n",
    "discriminator loss: 0.7016333341598511\n",
    "adversarial loss: 0.7507100105285645\n",
    "5700\n",
    "discriminator loss: 0.6917062997817993\n",
    "adversarial loss: 0.7270922660827637\n",
    "5800\n",
    "discriminator loss: 0.9430426359176636\n",
    "adversarial loss: 0.8752700090408325\n",
    "5900\n",
    "discriminator loss: 0.6874385476112366\n",
    "adversarial loss: 0.8012410998344421\n",
    "6000\n",
    "discriminator loss: 0.6968650817871094\n",
    "adversarial loss: 0.7524433135986328\n",
    "6100\n",
    "discriminator loss: 0.6927639245986938\n",
    "adversarial loss: 0.794948399066925\n",
    "6200\n",
    "discriminator loss: 0.6874150037765503\n",
    "adversarial loss: 0.7202481031417847\n",
    "6300\n",
    "discriminator loss: 0.6986126899719238\n",
    "adversarial loss: 0.7356060743331909\n",
    "6400\n",
    "discriminator loss: 0.6852483153343201\n",
    "adversarial loss: 0.7663735151290894\n",
    "6500\n",
    "discriminator loss: 0.6887850761413574\n",
    "adversarial loss: 0.7076794505119324\n",
    "6600\n",
    "discriminator loss: 0.6940613985061646\n",
    "adversarial loss: 0.7375900149345398\n",
    "6700\n",
    "discriminator loss: 0.6813197135925293\n",
    "adversarial loss: 0.7350896596908569\n",
    "6800\n",
    "discriminator loss: 0.7118894457817078\n",
    "adversarial loss: 0.7981077432632446\n",
    "6900\n",
    "discriminator loss: 0.7007394433021545\n",
    "adversarial loss: 0.775351881980896\n",
    "7000\n",
    "discriminator loss: 0.6933799982070923\n",
    "adversarial loss: 0.7448049783706665\n",
    "7100\n",
    "discriminator loss: 0.6985875964164734\n",
    "adversarial loss: 0.774384617805481\n",
    "7200\n",
    "discriminator loss: 0.6927677392959595\n",
    "adversarial loss: 0.7309372425079346\n",
    "7300\n",
    "discriminator loss: 0.6833792924880981\n",
    "adversarial loss: 0.8097614049911499\n",
    "7400\n",
    "discriminator loss: 0.706078827381134\n",
    "adversarial loss: 0.7577338814735413\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "indonesian-cooler",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47\n"
     ]
    }
   ],
   "source": [
    "print(step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fossil-mongolia",
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator loss: 3.460803508758545\n",
    "adversarial loss: 1.1308232927831341e-07\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
